{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratham-ja/LLM_engineering/blob/main/quantization_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "##An introduction quantization,LoRA and QLoRA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14UgrL83gcg"
      },
      "source": [
        "## A reminder of 2 important pro-tips for using Colab:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "The top of every colab has some pip installs. You may receive errors from pip when you run this, such as:\n",
        "\n",
        "> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
        "\n",
        "These pip compatibility errors can be safely ignored; and while it's tempting to try to fix them by changing version numbers, that will actually introduce real problems!\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "In the middle of running a Colab, you might get an error like this:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "This is a super-misleading error message! Please don't try changing versions of packages...\n",
        "\n",
        "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
        "\n",
        "1. Runtime menu >> Disconnect and delete runtime\n",
        "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
        "3. Connect to a new T4 using the button at the top right\n",
        "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
        "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
        "\n",
        "And all should work great - otherwise, ask me!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
        "!pip install -q datasets requests peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "# Hyperparameters for QLoRA Fine-Tuning\n",
        "## these are hyperparameters of Qlora value of R ,value of alpha ,target modules\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Log in to HuggingFace\n",
        "\n",
        "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
        "\n",
        "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx70lHF33TIa"
      },
      "source": [
        "what the below code is doing :\n",
        "using model class wrapper, casual llm means models used for next word prediction likt gpt,llama,.pretrained dowloads model weights and configurations returns the pytorch object as base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Trying out different Quantization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322,
          "referenced_widgets": [
            "b7a0fcbfcea847f4ae67fe152022eaa8",
            "64152dc99d564c4b84dc0526ec665d56",
            "2821b05255dd4b2dbe2f29372149cf90",
            "4956c753f7ff4dd1ad704e26dc9b3f42",
            "ff602f8eb552457d8b845233af048e20",
            "65224f563e424bf29f302c977975e864",
            "09fb36dc5c5143ca9a1d767bb0b321d9",
            "0a645ab5066445dabcddb2b65fc8ed45",
            "88967e7880a94c0289d614501b6d3992",
            "ebe69d3c569b495984409d2fa0cbe655",
            "0ce183bb519848c6b77b5659e01af7d2",
            "e0248585be6f4c13b0940924f4c6c861",
            "1099d01e5e9d427ab9a0b06591828605",
            "c489b4f9954d41c8a8e16b7b3753aa68",
            "ee3830b208624b5d91beb71a5308a53a",
            "4ee53df3a0694b7db000847949659398",
            "22009ade15c2459ea238720656bdd7db",
            "a2b74da13b9046ab8c1c675e24fc2bbf",
            "e6141d54ff174958b06b1d4883957602",
            "091c9a7440dc48a5acb9d3be11b1fc94",
            "fdee0af0cc0a4373975d5df9ddb191ca",
            "851c11d1d5484775bdc60b7a1449a18f",
            "00579f1286f4489baf55f7f5c0e022a4",
            "98a4bd4ad1d04b2fb348eda9ef3cac27",
            "8817f15a16ea4521b1f496732b693b65",
            "4850a5d2a659453f88917ddd9699b47b",
            "495f5d78f4404f9b9dee518445bc57ae",
            "4fdecc9eb6ef4d13ad530935cf86b90a",
            "f56e9064ed9a47d7b2e04f3d070177dc",
            "4b24d6b1b9994416b0e50e607f29d23d",
            "c1b0e33462174805aaf953adc3807538",
            "e29526cec37c48c8b483a373edb5cd51",
            "96f84d2489374e91bc18b22c0e5987bd",
            "ade97e7a1e2d4103a5916af69be6ec42",
            "b4a8e3452bb24d85aa8ad39ce92a8892",
            "ceec9d2f218941379120b000724b9ae2",
            "5cd01efaa6f1402996d0248df60d8581",
            "1b4112d61e9d46d799e2dc51bb162076",
            "8ca2b475c6eb43898178137b131e5b80",
            "42b5c3f0f6de4a05a7359bc0ff62525a",
            "77065b0ffabe4b159a11a73264a7953d",
            "49e53f8388fc4b97b82d2259209d682c",
            "68d2aa98f1a145d5a5b6d4d7164bc467",
            "2ab58b3a45054df3bcec8cba21c697fc",
            "5f779725a3f24c089b534eca17751b96",
            "f6bcd86e73204e32a0b65cfa15d1f16c",
            "48ca57110d0644da84d9f9bf9aa8bd80",
            "bb38049a12594c31a725f58ead52b04a",
            "7bf3586322404f0fb42042e20756efb6",
            "4fa4c05a3fc64b1eb1a90c148f798ee5",
            "12bc0a4f77584e82a13aab43c1e8484b",
            "4f076774ac044c039e170ab8efcdb904",
            "69b6251ecce442f1bfb7a8713edb9daf",
            "3fc41a8d306346a89e6d57f44dea5bf7",
            "49a9bed2ae644d2c99bdad5354a94fc0",
            "f67eb01e7d214055a34b6c8b3b18b136",
            "20428ed9ecb245279b54fdc68eb57b19",
            "5429246c4b4f4ce88dfe5f30409db153",
            "9d61909941f947f9a652f130dd89c8a1",
            "95b00c98e1a44c56b9d4db0d5012754e",
            "0e9faea1b11d452cba323b877e1e5818",
            "55aa6c69100b45b592661c5cca28b0ed",
            "461b458f76224f7aa45f2645c7e63836",
            "7903ba177fe5495e8f4d78ceae1b7c25",
            "5271d4c5cd4e4cf08cf61d9d1de6329e",
            "ba634ae9ee3541bb840d04e1bc7f9184",
            "609ac6118467483ca8b4da9c9a1d18ae",
            "ac772b5a4d354187a1dc37b39e37eeca",
            "a5ace72eba3e4bffa1e08afe3addd029",
            "af86d813fa9d482da237c52be24205a6",
            "8e8f5bb048db4047b64991b331c77916",
            "cac5f36f31fb4bee84e3a28e21343249",
            "6a45be6b71784bf2b6fd70c5f6be340e",
            "f75767d666a544babceb24844f2e12fb",
            "b87ccd0998fa417ca05642d2ce9c4eca",
            "ee3359997fb746e28fa33eba07f7192b",
            "156d0b1cb87b43e4811352de6f9094dd",
            "5ca1dd11c099464bbcab3f85a11e7a72",
            "bbb3fe5d56ee4a9fa90478b4a333dc9c",
            "c143c1d0e43449dd9302375e42ee7cfb",
            "bd2f5f5a42b84677b1a9620291fad983",
            "dd24ae375fe14af887044029b1514de1",
            "230db2795df1482d8a1991a63c7943fc",
            "13d37474b1824c2fa60725b3bb7c2c21",
            "8c0ac9a3dd874bc19b805048f8a963ee",
            "319529cf0b6843aea4cc8c419f6e26a9",
            "44491817a0ea414489326f8baf44bb72",
            "02f2fe2a08ea423a849ac078c69f4301",
            "f8271004a69a46a784d2b62f1aab4457",
            "cf152bafadb24e9598ece860d7d01f7b",
            "95fe7a10c32c4c9d86e82981dfb9ce3e",
            "df90060be48547ed86c2eb93b949f629",
            "44dc5fe5e8fb4ca3ad6edf532bee7ab3",
            "ad4fdf71c6cb4f31a12e6498b935a5bf",
            "90aced17a11c40eaa67dd00edd60c65b",
            "85f3949eca094b7c878a9599dcad2426",
            "ae8d47bcb9ff4a57a10fd0a05cfa5607",
            "f18d01a8144b412eabf1cf6702a2e81a",
            "c30a5c25aa894a639600a110a94089de"
          ]
        },
        "id": "ElUPTnB28iNK",
        "outputId": "eed7eb75-c768-4c1d-b413-65eb83b2e1a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7a0fcbfcea847f4ae67fe152022eaa8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0248585be6f4c13b0940924f4c6c861",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00579f1286f4489baf55f7f5c0e022a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ade97e7a1e2d4103a5916af69be6ec42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f779725a3f24c089b534eca17751b96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f67eb01e7d214055a34b6c8b3b18b136",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "609ac6118467483ca8b4da9c9a1d18ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ca1dd11c099464bbcab3f85a11e7a72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8271004a69a46a784d2b62f1aab4457",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        }
      ],
      "source": [
        "# Load the Base Model without quantization\n",
        "#\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VcoS_DY9YSG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6gJWw3r86KQ",
        "outputId": "deedfaaf-48da-4b0a-a7ae-36132f0ed41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory footprint: 32.1 GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8HDA0vo4M2I"
      },
      "source": [
        "This is beacuse we have 8 billion parameters and each is a floating point with 4 bytes memory and hence 8B*4 bytes = 32 gb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVf8hf6S88C9",
        "outputId": "ae6d1016-dff9-4fa7-eadf-87b07d017264"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qKpd144mVg"
      },
      "source": [
        "as you can see there are variuos layers of neurons embedding layer ,k,q all these are self attention blocks and then multi layer perceptrons ,normalization layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv-00uzNFPOe"
      },
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (installs and imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d925da61fedd47238750bacdfde440a5",
            "72ff431e41124066b781ad612ca3f481",
            "a0dd6057f6b547b59a51addf3d0a6ea1",
            "db48d27d5bfa4a77806fc14950a2e4b4",
            "d41d51b321004b5296e3b070ec47aced",
            "3c1161dabbdb4070a5e015b530fbe184",
            "43c0876df7ac4d138dd0909efda65730",
            "c16baac837534158a01595481b36dd28",
            "3ffab2c030e0463aa2ed6102fb542c63",
            "15373fdbc81240f2bc7057b28fd35788",
            "53543ce5d607486e881a52d0ccd36e60"
          ]
        },
        "id": "7ycR0B4CzUSJ",
        "outputId": "9c1ecc0e-1c74-4722-9f56-9660fed8bdf8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d925da61fedd47238750bacdfde440a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the Base Model using 8 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-rmCJVQ7j6b"
      },
      "source": [
        "the above configurations are used for qunatization loading parameters as 8 bit values and we are just passing quant_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVErveOYFOXW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FDztdnv0RCq",
        "outputId": "58c21033-dcfc-4bbc-a145-a58067e3e608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory footprint: 9.1 GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewquzkDq8YcF"
      },
      "source": [
        "8 bits = 1 byte * 8 billion = 8gb parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvqOxYfk0RnY",
        "outputId": "4ecc8691-8e3a-4e5b-8a5c-7fd937f9903c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7-0OfSEycl"
      },
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "33397c29394d4988b986801e37f57d45",
            "091946e0d8f549c2bc23483a2cc02a72",
            "f1c91b861c7c437e8341d805960030a2",
            "d6fa7383a63c4bbdb3f4c54c5cf15117",
            "367346758d924ed1bc64b3b45d6e0dad",
            "056f0b32a0204c6e882a21ac25d736e8",
            "6054dec62fa84831a62693589f3c8dc8",
            "c589c41528074af98b91f609b258f2b2",
            "6bfd4a0cfbde4591a2a40ef5e82cce5d",
            "4ecb57b85ec64bc48a81a462f1e034b9",
            "5067b28b54884c508f7f73ea158da94c"
          ]
        },
        "id": "R_O04fKxMMT-",
        "outputId": "6ad5488f-b112-45d0-d147-e8954b24cc20"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33397c29394d4988b986801e37f57d45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the Tokenizer and the Base Model using 4 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FfMJ2JbzEr3",
        "outputId": "12eeef4b-2c60-49ee-d42d-5ff3fe682bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory footprint: 5.59 GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsM4QH9G-our",
        "outputId": "c50f7361-ce94-4475-8e58-45fd13417054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
          ]
        }
      ],
      "source": [
        "print(type(base_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8H5Wpv_-vF-",
        "outputId": "da17afbd-fc1f-486d-9400-37bf10d50a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4540600320\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjp1EHH10WTb"
      },
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSpavkXo1KOI"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3kXoy3w1oMF"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2PL1nlM1tM1"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIaqo-gyBQRh",
        "outputId": "7e62105b-1d09-404b-a5e4-e4fe61da32d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of params: 27,262,976 and size 109.1MB\n"
          ]
        }
      ],
      "source": [
        "# Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
        "# These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
        "# Let's count the number of weights using their dimensions:\n",
        "\n",
        "# See the matrix dimensions above\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "# Each layer comes to\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "# There are 32 layers\n",
        "params = lora_layer * 32\n",
        "\n",
        "# So the total size in MB is\n",
        "size = (params * 4) / 1_000_000\n",
        "\n",
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKnCsfUQBG-P"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azkRqnKRRcZL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}